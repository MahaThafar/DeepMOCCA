{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from pycox.models import CoxPH, LogisticHazard\n",
    "from pycox.evaluation import EvalSurv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, BatchNorm,  TopKPooling, global_add_pool, SAGEConv, GraphConv, SAGPooling\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "import rdflib as rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N2021e0528a354755a503d06aaef9fd21 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "# Import the RDF graph for PPI network\n",
    "g = Graph()\n",
    "g.parse(\"data/rdf_string.ttl\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen={}\n",
    "done={}\n",
    "ei=[[],[]]\n",
    "ii=0\n",
    "def get_name(raw):\n",
    "    index=raw.rfind('_')\n",
    "    return raw[index+1:-1]\n",
    "for i,j,k in g:\n",
    "    sbj=get_name(i.n3())\n",
    "    obj=get_name(k.n3())\n",
    "    if sbj not in seen:\n",
    "        seen[sbj]=ii\n",
    "        ii+=1\n",
    "    if obj not in seen:\n",
    "        seen[obj]=ii\n",
    "        ii+=1\n",
    "    ei[0].append((seen[sbj]))\n",
    "    ei[1].append((seen[obj]))\n",
    "# for s in seen:\n",
    "#     print(s,seen[s])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j,k in g:\n",
    "    sbj=get_name(i.n3())\n",
    "    obj=get_name(k.n3())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a dictionary that maps protiens to their coresponding genes by Ensembl database\n",
    "import pickle\n",
    "f=open('data1/ens_dic.pkl','rb')\n",
    "dicty=pickle.load(f)\n",
    "f.close()\n",
    "dic={}\n",
    "for d in dicty:\n",
    "    key=dicty[d]\n",
    "    if key not in dic:\n",
    "        dic[key]={}\n",
    "    dic[key][d]=1\n",
    "# print(len(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary from ENSG -- ENST\n",
    "d = {}\n",
    "with open('data1/prot_names1.txt') as f:\n",
    "    for line in f:\n",
    "        tok = line.split()\n",
    "        d[tok[1]] = tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and pre-process methylation data\n",
    "def myth_data(fname):\n",
    "    f=open(fname)\n",
    "    line=f.readlines()\n",
    "    f.close()\n",
    "    output=[[0,0,0,0] for j in range(ii+1)]\n",
    "    for l in line:\n",
    "        temp=[]\n",
    "        trans,myth=l.split('\\t')\n",
    "        temp=trans.split(';')\n",
    "        myth=float(myth)\n",
    "        for x in temp:\n",
    "            index=x.find('.')\n",
    "            if index<1:\n",
    "                index=len(x)\n",
    "            x=x[:index]\n",
    "            if x in d:\n",
    "                gen = d[x]\n",
    "            if gen in seen:\n",
    "                output[seen[gen]][0]=myth\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gene expression files and Pre-process them \n",
    "import gzip\n",
    "def exp_data(fname,cnvname,vcfname,output):\n",
    "    f=gzip.open(fname,'rt')\n",
    "    line=f.readlines()\n",
    "    f.close()\n",
    "    for l in line:\n",
    "        gene,exp=l.split('\\t')\n",
    "        prev=gene\n",
    "        index=gene.find('.')\n",
    "        if index<1:\n",
    "            index=len(gene)\n",
    "        gene=gene[:index]\n",
    "        exp=float(exp)\n",
    "        if gene in seen:\n",
    "            output[seen[gene]][1]=exp\n",
    "# Import CNV files and Pre-process them             \n",
    "    f=open(cnvname)\n",
    "    line=f.readlines()\n",
    "    f.close()    \n",
    "    for l in line:\n",
    "        gene,cnv=l.split('\\t')\n",
    "        prev=gene\n",
    "        index=gene.find('.')\n",
    "        if index<1:\n",
    "            index=len(gene)\n",
    "        gene=gene[:index]\n",
    "        cnv=int(cnv)\n",
    "        if gene in seen:\n",
    "            output[seen[gene]][2]=cnv                        \n",
    "# Import VCF files and Pre-process them            \n",
    "    f=open(vcfname)\n",
    "    line=f.readlines()\n",
    "    f.close()    \n",
    "    for l in line:\n",
    "        gene,score=l.split('\\t')\n",
    "        score=float(score)\n",
    "        if gene in seen:\n",
    "            output[seen[gene]][3]=score\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture\n",
    "from torch_geometric.data import Data\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.conv1 = GCNConv(4,32)\n",
    "        self.pool1 = SAGPooling(32)\n",
    "        self.conv2 = GCNConv(32,16)\n",
    "        self.fc1 = nn.Linear(16,8)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, perm, score = self.pool1(x, edge_index, None, batch)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = gmp(x, batch)\n",
    "        b=data.y.shape[0]\n",
    "        x=x.view(b,-1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CoxPH(MyNet().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    loss_=torch.nn.MSELoss()\n",
    "    for data in train_loader:\n",
    "        label=data.y\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.type(torch.FloatTensor)\n",
    "        loss = loss_(output, (data.y.view(output.shape[0],1)).type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        loss_all += loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "def test(loader,dataset,thresh):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for data in (loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            b,_=output.shape\n",
    "            data.y=data.y.reshape(output.shape[0],1)\n",
    "            correct+=sum((output>=thresh) & (data.y>0.0))+sum((data.y<1.0) & (output<thresh))\n",
    "        return float(correct) / float(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_types = [\"BRCA\",\"GBM\",\"OV\",\"LUAD\",\"UCEC\",\"KIRC\",\"HNSC\",\"LGG\",\"THCA\",\"LUSC\",\"PRAD\",\"SKCM\",\"COAD\",\"STAD\",\"BLCA\",\"LIHC\",\"CESC\",\"KIRP\",\"SARC\",\"LAML\",\"PAAD\",\"ESCA\",\"PCPG\",\"READ\",\"TGCT\",\"THYM\",\"KICH\",\"ACC\",\"MESO\",\"UVM\",\"DLBC\",\"UCS\",\"CHOL\"]\n",
    "for i in range(len(can_types)):\n",
    "    clin = [] # for clinical data (i.e. number of days to survive)\n",
    "    feat_vecs=[] # list of lists ([[patient1],[patient2],.....[patientN]]) -- [patientX] = [gene_expression_value, methylation_value, VCF_value, CNV_value]\n",
    "    # file that contain patients ID with their coressponding 4 differnt files names (i.e. files names for gene_expression, methylation, VCF and CNV)\n",
    "    f=open('data1/intersect_five_'+can_types[i]+'.tsv')\n",
    "    lines=f.readlines()\n",
    "    f.close()\n",
    "    lines=lines[1:]\n",
    "    count=0\n",
    "    for l in tqdm(lines):\n",
    "        try:\n",
    "            l=l.split('\\t')\n",
    "            # Check if all 4 files are exist for a patient (that's because for some patients, their survival time not reported)\n",
    "            if os.path.isfile('cancer_types/TCGA-'+can_types[i]+'/clinical/'+l[2]) and os.path.isfile('cancer_types/TCGA-'+can_types[i]+'/myth/'+l[3]) and os.path.isfile('cancer_types/TCGA-'+can_types[i]+'/exp_norm/col/'+l[len(l)-1].rstrip()) and os.path.isfile('cancer_types/TCGA-'+can_types[i]+'/cnv/'+l[0]+'.txt') and os.path.isfile('cancer_types/TCGA-'+can_types[i]+'/vcf/output/'+'OutputAnnoFile_'+l[1]+'.hg38_multianno.txt.dat'):\n",
    "                temp=l[2]\n",
    "                f=open('cancer_types/TCGA-'+can_types[i]+'/clinical/'+temp)\n",
    "                content=f.read().strip()\n",
    "                f.close()\n",
    "                clin.append(content)\n",
    "                temp_myth=myth_data('cancer_types/TCGA-'+can_types[i]+'/myth/'+l[3])\n",
    "                feat_vecs.append(exp_data('cancer_types/TCGA-'+can_types[i]+'/exp_norm/col/'+l[len(l)-1].rstrip(),'cancer_types/TCGA-'+can_types[i]'+/cnv/'+l[0]+'.txt','cancer_types/TCGA-'+can_types[i]+'/vcf/output/'+'OutputAnnoFile_'+l[1]+'.hg38_multianno.txt.dat',temp_myth))\n",
    "            else:\n",
    "                print('Not exist!')\n",
    "\n",
    "        except:\n",
    "            count+=1\n",
    "    # Normlize survival time, node features have been normalized separately before using min-max normalization\n",
    "    labels = [float(i) for i in clin]\n",
    "    label = []\n",
    "    for i in range(len(labels)):\n",
    "        label.append(labels[i]/max(labels))\n",
    "    # Train by batch\n",
    "    dataset=[]\n",
    "    edge=torch.tensor(ei,dtype=torch.long)\n",
    "    i=0\n",
    "    for e in range(len(feat_vecs)):\n",
    "        x=torch.tensor(feat_vecs[e],dtype=torch.float)\n",
    "        labell = label[e]\n",
    "        dataset.append(Data(x=x,edge_index=edge,y=torch.tensor([labell])))\n",
    "    # Split the dataset\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    remain = len(dataset) - train_size\n",
    "    test_size = int(0.15 * len(remain))\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train(epoch)\n",
    "        train_acc = test(train_loader,train_dataset,0.5)\n",
    "        test_acc = test(test_loader,test_dataset,0.5)\n",
    "        print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
    "              format(epoch, loss, train_acc, test_acc))\n",
    "    # Compute the evaluation measurements\n",
    "    for t in test_loader:\n",
    "        preds+=(model(t.to(device)).detach()).tolist()\n",
    "        trues+=t.y.tolist()\n",
    "    preds=[x for x in preds]\n",
    "    c_index = EvalSurv(trues, preds).concordance_td()\n",
    "    mse = mean_squared_error(trues, preds)\n",
    "    rmse = math.sqrt(mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
